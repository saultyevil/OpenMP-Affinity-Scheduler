\documentclass[11pt, a4paper]{article}

\usepackage{fancyhdr} 					  % header and footer tools for the page style
\usepackage{hyperref} 					  % adds hyperlinks to navigate the document
\usepackage{graphicx} 					  % provides extra arguments for \includegraphics[keyvals]{imagefile}
\usepackage[margin=2.2cm]{geometry}		  % modify the geometry of the document
\usepackage[hang,flushmargin]{footmisc}   % modify footnotes - used here to remove the footnote indentation
\usepackage{minted}                       % insert code

% remove the footnote rule
\renewcommand*\footnoterule{}

\begin{document}

	\title{Advanced Computational Methods II: OpenMP Parallelisation}
	\author{Edward John Parkinson}
	\maketitle	
	
	\section{Introduction}
		The aim of this report is to compare the performance of OpenMP's (OMP) parallel loops scheduling methods, and also, an alternative affinity scheduling algorithm which has been developed as part of this coursework. The different scheduling methods are benchmarked on the ARCHER supercomputer, typically using six threads. The performance gain/losses of the number of chunks per thread is explored using 1, 2, 4, 8, 16, 32 and 64 chunks for the \texttt{static}, \texttt{dynamic} and \texttt{guided}  schedules. Finally, the schedules which result in the greatest speed up are examined in further detail, where the speed up gained with the number of threads in use is compared.
		
		The schedules are benchmarked against two loops; exerts of these loops can be found in Fig. \ref{fig:loop_code}. The performance is benchmarked through comparison of the runtime required to execute the loops 100 times. Both loops provide tests for loops with unbalanced workloads as the size of inner loops varies in size depending on the current iteration of the outer loop. However, whereas loop 1 provides a fairly predictable imbalance (as the inner loop gets smaller for larger iterations of the outer loop), loop 2's imbalance is less predictable; the work imbalance for the first inner loop for both loops is shown graphically in Fig \ref{fig:loop_balance}. Hence, due to the difference in work balancing, we can expect both loops to perform differently for different scheduling methods.
		 
		\textit{All code will be compiled using the \texttt{-O3} optimisation flag. The average runtime for the serial execution of loop 1 is 0.045 seconds and loop 2 is 0.604 seconds, using the Cray Compiler (CC) on ARCHER. All runtimes for the scheduling methods used are computed by taking the average runtime of ten runs.}

		\begin{figure*}
			\begin{minted}[frame=lines, framesep=2mm, gobble=4]{C}
				void loop1(void)
				{
					// parallelise this outer loop
					for (int i = 0; i < N; i++)
					{
						for (int j = N - 1; j > i; j--)
						{
							a[i][j] += cos(b[i][j]);
						}
					}
				}
			\end{minted}
			
			\begin{minted}[frame=lines, framesep=2mm, gobble=4]{C}
				void loop2(void)
				{
					double rN2 = 1.0/(double)(N * N);
	
					// parallelise this outer loop
					for (int i = 0; i < N; i++)
					{
						for (int j = 0; j < jmax[i]; j++)
						{
							for (int k = 0; k < j; k++)
							{
								c[i] += (k + 1) * log (b[i][j]) * rN2;
							}
						}
					}
				}
			\end{minted}
			\caption{The two benchmark loops being used. Both loops provide an unbalanced workload.}
			\label{fig:loop_code}
		\end{figure*}
	
		\begin{figure}
			\centering
			\includegraphics[scale=0.33]{./results/loop_balance.pdf}
			\caption{The y-axis shows the number of iterations to be done for the first inner loops of loop 1 and loop 2. Loop 1 has a predictable balance whereas loop 2 is less predictable. For low values of \texttt{i} in loop 2, the number of iterations is generally large but as \texttt{i} increases, the frequency of a large amount of large iterations occurring decreases.}
			\label{fig:loop_balance}
		\end{figure}
			
	\section{OpenMP's Default Scheduler}
		In this section, I will discuss my findings on the execution times for the different scheduling methods provided by OMP. All tests were ran on ARCHER, typically using six threads. To enable parallel computation, the OMP command, \mint{c}{#pragma omp parallel for default(none), schedule(runtime), ...} \noindent is used on the outer-most loops of each loop function; see Fig. \ref{fig:loop_code}. Using \texttt{schedule(runtime)} allows the choice of schedule to be decided at code runtime, instead of at compile time, by setting the OMP environment variable \texttt{OMP\char`_SCHEDULE=schedule,chunk\char`_size}, where \texttt{schedule} is either \texttt{static}, \texttt{dynamic}, \texttt{guided}  or \texttt{auto}. If \texttt{chunk\char`_size} is not set, a default value of $1$ is used instead, except in the case of \texttt{static} where the work is split as equally as possible between threads.
	
		\subsection{Static and Auto}
			The first two tests conducted used the \texttt{static}~ schedule with no defined chunksize and the \texttt{auto}~ schedule. When no chunksize is defined for \texttt{static}, the iterations are split as evenly as possible between all threads, this is efficient for evenly balanced loops but results in inefficient thread idle time in imbalanced loops. The \texttt{auto} scheduler allows the compiler to decide the scheduling method to use and also allows the possibility of the scheduling method to change during runtime. Thus, if a loop has a large amount of iterations, this allows the scheduling to evolve to a method with good workload balance and low overheads. However, one must be careful when using \texttt{auto} as some compilers, such as \texttt{gcc}, are unable to decide on a scheduling method and will map \texttt{auto} to another scheduling method, such as \texttt{static}. 
			
			The results for these two tests are shown in Fig. \ref{fig:static_auto}.
			
			\begin{figure}
				\centering
				\includegraphics[scale=0.45]{./results/static_auto.pdf}
				\caption{Runtime for loop 1 and 2 using serial execution and parallel execution with \texttt{static} or \texttt{auto} scheduling methods.}
				\label{fig:static_auto}
			\end{figure}
			
			\subsubsection{Static} \label{sect:static_np}
				With \texttt{static} scheduling, there is an increase in runtime for loop 1. This is due to the imbalanced workload of each thread and the overhead related to the scheduler. Some threads require more time to finish their work, whereas the other threads will have completed their assigned work. This results in the completed threads having to wait for the other threads to finish before all the threads can resynchronise at the end of the parallel region; this is an inefficient use of the hardware available. The resulting overhead of scheduling the loop and synchronising the threads at the end results in an increased runtime over the serial execution of loop 1. However, \texttt{static} scheduling results in a decreased runtime for loop 2; a performance gain. This will be due to the balance of the workload between the threads, as well as the execution time of the loop being larger than the overhead to schedule. The thread which is assigned the first 30 iterations of the parallelised outer loop will take longer to complete its assigned work than the other threads, but, as shown in Fig. \ref{fig:loop_balance}, the distribution of the longer inner loops is such that the work balance for the other threads is only mildly imbalanced. This results in more efficient computation when using the \texttt{static} schedule. However, for both loops with the \texttt{static} we do not see a significant decrease in runtime considering the number of threads available. For example, with six threads being used by ARCHER, the runtime for loop 2 is decreased by around 33\%. This tells us that the \texttt{static} schedule is inefficient\footnote{The inefficiency will be due to some threads being idle whilst other threads are still completing their assigned work.} and inadequate to use for both loops.
				
			\subsubsection{Auto}
				With the \texttt{auto} schedule, the runtime for both loops is decreased. However, the performance gain for loop 1 is much smaller than the performance gain for loop 2. The lower performance for loop 1 is most likely due to the overheads of finding and using the best schedule to use for loop 1; the extra runtime introduced due to any overheads related to the scheduling of work will be comparable to the runtime required to execute the code. The runtime for loop 2 is decreased by just over 50\%, which indicates that \texttt{auto} was able to find a scheduling method with low overheads. However, it must be noted that, as will be seen later in \S\ref{sect:chunksize}, the performance gain of the \texttt{auto} schedule is not as great as when compiled with a suitable schedule, such as \texttt{dynamic}.
		
		\subsection{Chunksize} \label{sect:chunksize}
			For these tests the chunksize for the \texttt{static}, \texttt{dynamic} and \texttt{guided} schedules is changed to investigate the relationship between runtime and chunksize. As before, the tests were conducted using six threads on ARCHER, with chunksizes of $1, 2, 4, 8, 16, 32, 64$.
			
			The results for these tests can be found in Fig. \ref{fig:chunksize}. 
			
			\begin{figure}
				\centering
				\includegraphics[scale=0.45]{./results/chunksize.pdf}
				\caption{The speed up ratio, $T_{1}/T_{chunk}$, against the chunksize of the \texttt{static}, \texttt{dynamic} and \texttt{guided} schedulers for loop 1 and loop 2. The solid red line, without any markers, represents the serial execution of each loop.}
				\label{fig:chunksize}
			\end{figure}
			
			\subsubsection{Loop 1} \label{sect:loop1_chunks}
				For loop 1, Fig. \ref{fig:chunksize} shows that for \texttt{guided} and \texttt{dynamic} schedules, changing the chunksize does not dramatically change the performance of the scheduling methods. However, it shows that changing the chunksize of the \texttt{static} schedule can drastically change the performance of this scheduling method. 
				
				When \texttt{static} is used, we can see a performance increase comparable to \texttt{dynamic} and \texttt{guided} for smaller chunksizes. However, when the chunksize is increased, the performance of the \texttt{static} scheduler is shown to decrease with chunksize, where with a chunksize of around 48, the performance of the parallel execution is worse than with serial execution ($T_{1}$). This decrease in performance is due to the poor work balance of loop 1, as mentioned earlier in \S\ref{sect:static_np}. For smaller chunksizes, the imbalance of the work does not severely effect the performance as, for example, with a chunksize of one with two threads, we can expect thread 0 to be assigned iteration 1 and thread 1 to be iteration 2, and so on alternating between threads. This will lead to a more balanced load for the two threads than for example with a chunksize of 32. For this chunksize, thread 0 will assigned to the first 32 iterations and thread 1 to the next 32, and so on. With larger chunksizes, the work balance between threads becomes more imbalanced and thus results in threads being idle whilst other threads are still executing. Thus for loops such as loop 1, we should be careful with using larger chunksizes for the \texttt{static} schedule. However, the overhead associated with scheduling for larger chunksizes is smaller than for smaller chunksizes, thus for loops with a large amount of iterations to be run in parallel, a compromise for chunksize will be required for execution time and the overhead required for the schedule.

				However, when \texttt{dynamic} and \texttt{guided} scheduling is used, there is a relatively constant level of performance when varying the chunksize. For both of these schedules, each thread executes a chunk of the iterations and then requests the next chunk of iterations to execute. However, whilst the chunksizes remain constant with the \texttt{dynamic} scheduler, the chunksize for the \texttt{guided} schedules begins as $n/p$ and becomes progressively smaller as the number of iterations left decreases, where the chunksize will limit itself to a specified minimum chunksize; however the last iteration may be smaller than this minimum chunksize. For example, if we start with a chunksize of 64 and state a minimum chunksize of eight, we can expect the final batch of chunks executed to be eight iterations large or smaller for the final chunk. Fig. \ref{fig:chunksize} shows that there is a minor performance difference between the two schedules. For smaller chunksizes, \texttt{guided} has a greater performance gain than \texttt{dynamic}, but as chunksizes increases, the performance gap between the two scheduling methods decreases. We can, in general, expect better performance for unbalanced loops from \texttt{guided} scheduling due to the larger chunksizes at the beginning of the scheduling reducing the overhead required, and the smaller chunks near the end ``filling in'' the remaining work to be done. This results in the \texttt{guided} schedule being especially good for loops where there is a large work imbalance between the first and finial iterations of a loop. However, \texttt{dynamic} scheduling is also efficient at dealing with imbalanced work loads, as can be seen by the small performance gap between \texttt{dynamic} and \texttt{guided}, as each thread is idle for less time, thus the scheduler makes more efficient use of the hardware available. However, for a small amount of iterations, the use of \texttt{dynamic} and \texttt{guided} scheduling may increase runtime as there is significantly more overhead required for these scheduling methods. 
				
				From these tests, it was thus found that the best schedule to use for loop 1 is the \texttt{guided} schedule, with a chunksize of four.

			\subsubsection{Loop 2} \label{sect:loop2_chunks}
				Compared to loop 1, loop 2 benefits greater from the scheduling methods used thus far. Fig. \ref{fig:chunksize} shows that there is no discernible performance difference as the chunksize is changed for the \texttt{guided} schedule, as with loop 1. However,  both the \texttt{static} and \texttt{dynamic} schedules performance vary greatly with chunksize, where both schedulers' performance declines rapidly once the chunksize advances past a ``sweet spot''.
				
				The \texttt{guided} schedule offers no real performance difference with the different chunksizes used. This will be due to how the \texttt{guided} schedule functions by starting off with large chunks to lower the overheads and reducing the chunksize, which helps when a loop is imbalanced. However, since loop 2 is not as imbalanced as loop 1, a small chunksize does not help balance the workload as well between threads and it will introduce a larger overhead.
				
				The \texttt{static} scheduler on the other hand provides a good performance boost when used with a chunksize of four; which peaks with a speed up ratio of 4.52. However, for chunksizes smaller and larger than four we see worse performance gain. For larger chunksizes, the lower performance will be due to poor work balance between the threads, as the thread assigned with the first 30 iterations of loop 2, which is the most loaded part of loop 2 (see Fig. \ref{fig:loop_balance}), will take longer to finish its assigned work which would result in idle thread whilst the thread is still executing. This results in inefficient computation and worse performance gain over other chunksizes and scheduling choices. We also see lower performance gain for smaller chunksizes which could be due to the overhead related to scheduling the smaller chunksizes. However, with small chunksizes, \texttt{static} still provides better performance than the \texttt{guided} schedule for all choices of minimum chunksize.
											
				Finally, the \texttt{dynamic} schedule provides the best performance gain for loop 2 when a chunksize of eight is used. However, the performance gap between the smaller chunksizes is very small and thus any chunksize below eight would be a good choice to use. When the chunksize is larger than eight, Fig. \ref{fig:chunksize} shows that the performance of \texttt{dynamic} scheduling decreases. The performance decrease occurs due to the lack of flexibility the scheduler has for larger chunksizes.  With larger chunksizes, the scheduler is not able to ``fill in'' the gaps, but instead must execute a large chunk before it can move onto the next chunk. This can result in threads executing less chunks than other threads resulting in poor work balance and inefficient use of the hardware available. However, with smaller sized chunks, threads will, in general, be executing chunks for less time and thus will be executing more chunks in general. This provides better a better and more flexible framework for managing the work balance between threads.
							
				For these tests, it was thus found that the best schedule to use for loop 2 is the \texttt{dynamic} schedule, with a chunksize of 8.
											
		\subsection{Number of Threads}
			In \S\ref{sect:loop1_chunks} and \S\ref{sect:loop2_chunks} it was found that the best schedule for six threads for loop 1 is \texttt{guided, 4} and \texttt{dynamic, 8} for loop 2. In this section, I compare the performance difference when using 1, 2, 3, 6, 12 and 24 threads on ARCHER. Both schedulers performance will be compared for both loops.
			
			The results for these tests are in Fig. \ref{fig:n_threads}.\
			
			\begin{figure}
				\centering
				\includegraphics[scale=0.45]{./results/speedup.pdf}
				\caption{The speed up ratio, $T_{1}/T_{p}$, against the number of threads of the \texttt{guided, 4} and \texttt{dynamic, 8} schedulers for loop 1 and loop 2. The red line corresponds to base performance, i.e. serial execution.}
				\label{fig:n_threads}
			\end{figure}
			
			\subsubsection{Guided, 4}
				The \texttt{guided} schedule with a chunksize of 4 was found to be the best schedule for loop 1 with six threads. However, we can see for loop 1 in Fig. \ref{fig:n_threads} that when we use less than six threads, the scheduler negatively impacts the execution time of the loop. The runs with three or less threads in use take longer to execute than the serial execution of the loop. This will be due to the overhead of scheduling and assigning threads with chunks and the synchronisation of threads. The overhead will be comparable to the runtime of each loop, thus for short loops, such as loop 1, we should be careful when using parallel methods to make sure we are using enough hardware to get a sufficient performance gain. %This could be due to CC being able to use sequential optimisations when the OMP \texttt{parallel for} directive is used. 
				In general, a workaround for this problem is often hard to find but setting shared variables to be private variables for the thread 
				can cure the issue. Fig. \ref{fig:n_threads} also shows for loop 1 that the number of thread does not follow a one-to-one linear relationship between speed up and the number of threads used. For example, we can see that when six threads are used, we see a speed up factor of 1.3 and when 24 thread are used we see a speed up of around 4.5. If there was a one-to-one relationship, we would expect to see a more significant performance gain than a speed up factor of 4.5. We will not see any significant speed up with loop 1 due to the short execution time of the thread and the overhead required to schedule and synchronise all of the threads. The overhead required will increase as the number of threads in use increases, however this increased overhead is offset by the fact that there are more threads executing chunks in parallel.
				
				However, when we use the \texttt{guided} schedule for loop 2, we see a speed up when two threads are in use which we didn't see for loop 1; note the fact that loop 2 is faster with \texttt{guided} with one thread is due to machine variation when the code was tested. The performance increase observed for when less threads are used is due to the fact that the overhead related to the synchronisation of the threads is small compared to the execution time of each chunk. We thus do not reach a scenario where the performance is impacted by ``death by synchronisation''.  We see again, as with loop 1, an increase in performance as the number of threads increases for the same reason that more chunks are being executed in parallel. 
			
			\subsubsection{Dynamic, 8}
				The \texttt{dynamic} schedule with a chunksize of 8 was found to be the best schedule for loop 2 with six threads. Fig. \ref{fig:n_threads} shows that there is a rapid increase in performance as the number of threads is increased. The performance increase can be attributed to better work balancing between threads due to how \texttt{dynamic} scheduling assigns a new chunk whenever a thread becomes idle. However, the performance gain beings to plateau when more than twelve threads are used. One of the reasons for the performance decrease could be due to cache contention and data affinity across threads. Cache contention is where more than one thread will be trying to access the same memory in cache at the same time. This causes threads to queue until they are able to access the memory resulting in less efficient computation. The performance could be improved by improving the data locality for each thread and by sharing variables over multiple caches, i.e. using more private variables. The performance decrease can also be due to the fact that we optimised chunksize for six threads. In general the best chunksize is related to the number of threads available, for example, if we tested chunksize using twelve threads, we could expect to find that \texttt{dynamic} is still the best schedule to use, but with a different chunksize.
				
				Fig \ref{fig:n_threads} again shows a ``death by synchronisation'' for loop 1 for the \texttt{dynamic} schedule, where the overhead related to scheduling and syncing the chunks and threads limits the performance. There is no performance gain for executing the loop in parallel until six or more threads is used. 
			
	\section{Affinity Scheduler}
		\subsection{Introduction} \label{sect:affinity_intro}
			The scheduler implemented for this coursework work in a similar fashion to the \texttt{dynamic} and \texttt{guided} schedulers from OMP. When implemented correctly, the affinity scheduler will distribute all available iterations to each thread as evenly as possible, known as a thread's local set. Each thread will then execute chunks of size $1/n_{threads}$ until there are no chunks left to execute in a thread's local set. The thread will then determine which of the remaining executing threads has the most iterations remaining in its set and take a fraction $1/n_{threads}$ to execute itself. By doing this, each thread should remain working until there are no iterations left.
			
			To implement this method correctly, the threads have to be synchronised after each thread has finished executing its assigned chunk. Thus, we will be introducing an overhead by doing this but as threads will remain working throughout the execution of the entire loop, we can expect that the affinity scheduler to provide a performance gain for most situations.	
			
		\subsection{Implementation}
			The affinity scheduler is contained within three functions; a main function containing the parallel region (\texttt{affinity\char`_loop}) and two smaller functions, one of which is used to assign work to a thread (\texttt{share\char`_iterations}) and the other which is used to find the thread which has the most remaining work (\texttt{find\char`_loaded\char`_thread}). The main function requires a single argument, which is a pointer to a function containing a loop. The loop function is required to have two integer arguments for a lower and upper boundary to iterate through. %Thus to use the affinity scheduler, one has to call the function where desired in their code with an appropriate pointer. 
			In it's current state, the scheduler returns \texttt{void}, but, in principle, it is simple enough to modify the main scheduling function to return a value if desired. Flow charts for the basic flow of the main and work assigning functions can be found in \S\ref{sect:appendix}.
			
			\subsubsection{Initialising the Parallel Region} \label{sect:parallel_region}
				The purpose of the \texttt{affinity\char`_loop} function is to initialise a parallel region and to control the work which each thread is doing. The first task of the function is to calculate the number of iterations each thread is expected to execute, $n_{local}$. This is achieved by dividing the total number of iterations $N$ by the number of threads, $n_{threads}$, where the number of threads is found by using the OMP function \texttt{omp\char`_get\char`_max\char`_threads()}. This is calculated as a \texttt{double} but the standard C function \texttt{ceil} is used to always \textit{round up} the value to an integer. This can sometimes result in $n_{threads} \cdot n_{local} > N$. However, this is accounted for later when iterations are being assigned to threads. With $n_{local}$ calculated, a parallel region is constructed which is followed by the assignment of the lower and upper boundaries of the iterations for each thread's local set. The lower limits, $L$, of each thread are calculated by multiplying the thread identification number, $P_{id}$, by the number of iterations each thread is required to execute,  $P_{id} \cdot n_{local}$. The upper limits, $U$, are calculated similarly by multiplying the number of iterations  by the next thread identification number $(P_{id} + 1) \cdot n_{local}$. %; the subtraction here avoids causing an overlap between threads resulting in extra iterations. 
				To account for the use of the \texttt{ceil} function when calculating $n_{local}$, the upper boundary values have the boundary condition that if $U > N$, then, $U = N$ \footnote{As array indexing in C begins at 0, in the scheduler we must be careful to use the limit $N-1$ instead.}.
				
				The lower and upper boundaries for each thread are stored in the arrays \texttt{thread\char`_lowers} and \texttt{thread\char`_uppers} respectively. These arrays are shared between all threads which allows each thread to access the corresponding array element of any thread to query how many iterations that thread has left to execute. There are $n_{threads}$ elements in each array and the thread identification number is the index for that thread in the array. Similarly, the current lower and upper boundary of the chunk each thread is executing is stored in the arrays \texttt{chunk\char`_lowers} and \texttt{chunk\char`_uppers} respectively, for the same reason as before. However, in general, each thread does not need to know the chunk each thread is currently executing and thus variables private to the thread can be used instead to control the chunk upper and lower boundaries. 
				
			\subsubsection{Sharing Work Between Threads} \label{sect:sharing_work}
				To control the work each thread is doing, a \texttt{while} loop is used in \texttt{affinity\char`_loop} which continuously iterates until the work sharing function \texttt{share\char`_iterations} returns the break condition. Inside the \texttt{while} loop, the loop function is executed using the chunk boundaries for the thread as set in \texttt{chunk\char`_lowers} and \texttt{chunk\char`_uppers}.  Initially when the scheduler begins, the value for \texttt{chunk\char`_lower} and \texttt{chunk\char`_upper} is set to zero for all threads. This results in the first execution of the loop function exiting immediately. When the loop function has finished executing, the code assigns the next (or first) chunk of iterations to be executed by the current thread (this is done in the function \texttt{share\char`_iterations}). For this to be successful, care has to be taken with how the threads are synchronised and how they communicate which iterations are still left to be executed; we have to make sure a thread does not execute the same chunk another thread has already executed or is still executing. One way to prevent threads from executing the same work is to use an OMP \texttt{critical} section. A \texttt{critical} section prohibits multiple threads from executing a block of code and thus only allows a single thread at once to execute that block. By doing this, we limit that only one thread at a time can find work to take from another thread, resulting in threads queueing to be assigned a chunk which has not yet been executed. Thus, the function \texttt{share\char`_iterations} which assigns chunks to the calling thread, is placed within a \texttt{critical} section so only one thread at once can be assigned a new chunk to execute.
				
				The first block of code in \texttt{share\char`_iterations} determines if the calling thread should execute iterations from its own local set, or if there are no more iterations left in its local set, if it should execute iterations from another thread's local set. This is determined by subtracting \texttt{thread\char`_lowers} from \texttt{thread\char`_uppers} with the relevant array indexes in use. To determine if the thread should execute work from it's own local set, the thread identification number, found using the OMP function \texttt{omp\char`_get\char`_thread\char`_num()}, is used as the index for the thread arrays.
				
				If there is no work left in a thread's local set, the function \texttt{find\char`_loaded\char`_thread} is called, which will find the thread with the most remaining iterations and will return the array index and number of iterations still left to execute. The most loaded thread is found by looping through the arrays \texttt{thread\char`_lowers} and \texttt{thread\char`_uppers} and subtracting one from the other. The thread index and number of iterations remaining are stored in an array \texttt{loaded\char`_thread} with the first element being the index of the loaded thread and next element being the number of iterations still left to be executed. The function \texttt{share\char`_iterations} then proceeds differently depending on if there are iterations still remaining in any thread's local set. \\
				
				\noindent \textbf{If there are iterations remaining:}
				
				When there are iterations remaining, the next step is to assign a new chunk of iterations for the thread to execute; this is done identically if the new chunk is from the thread's own set or if from another thread's set. The size of the new chunk is determined by multiplying the number of remaining iterations by the fraction $1/n_{threads}$ \footnote{Note again that this value is \textit{always} rounded up to avoid the situation where the number of iterations to take will round down to zero and thus results in the scheduler never assigning the last iteration to be executed.}. This results in the assigned chunks decreasing in size as the number of iterations also decreases. To assign a new chunk to a thread, the arrays \texttt{chunk\char`_lowers}, \texttt{chunk\char`_uppers} and \texttt{thread\char`_lowers} are updated. The value of \texttt{chunk\char`_lower} for the calling thread is set as the value for \texttt{thread\char`_lower} for the thread where iterations are being taken from; this sets the chunk to start iterating from the first iteration which has not yet been executed. Subsequently, the value for \texttt{chunk\char`_upper} for the calling thread is the value \texttt{chunk\char`_lower} incremented by the chunksize. The value contained in \texttt{thread\char`_lowers} for the thread where iterations are being taken from is also incremented by the chunksize. This updates where the next chunk for this thread should start from and also updates how many iterations are left for the thread. The function \texttt{share\char`_iterations} then returns \texttt{CONTINUE} to indicate to the \texttt{while} loop in \texttt{affinity\char`_loop} to continue iterating and thus the loop function will execute again with the new chunk lower and upper boundaries. This process will repeat until there are no iterations left.\\
			
				\noindent \textbf{If there are no iterations remaining:}
			
				When there is no work remaining, the function \texttt{find\char`_loaded\char`_thread} will return \texttt{STOP} for both elements of \texttt{loaded\char`_thread} to \texttt{share\char`_iterations}. \texttt{STOP} is then returned to \texttt{affinity\char`_loop}, which breaks the \texttt{while} loop and thus all the work is done. The parallel region is then exited and the main program will continue.
				
		
		\subsection{Performance}
			To test the performance of the developed affinity scheduler, the speed up factor $T_{1}/T_{p}$ of the affinity scheduler is compared to the performance of the \texttt{guided, 4} and \texttt{dynamic, 8} schedulers using 1, 2, 3, 6, 12 and 24 threads on ARCHER. The results of this can be found in Fig. \ref{fig:affinity_comparison}.
			
			\begin{figure}
				\centering
				\includegraphics[scale=0.45]{./results/affinity_comparison.pdf}
				\caption{The speed up ratio, $T_{1}/T_{p}$, against the number of threads of the \texttt{guided, 4},  \texttt{dynamic, 8} and the developed affinity schedulers for loop 1 and loop 2. The red line corresponds to base performance, i.e. serial execution.}
				\label{fig:affinity_comparison}
			\end{figure}
		
			\subsubsection{Loop 1} \label{sect:loop1_affinity}
				As with the \texttt{guided} scheduler, there is no performance increase using the affinity scheduler until six threads or more are used. This once again is due to ``death by synchronisation'' where there is no benefit gained from running in parallel due to the overhead of scheduling and assigning chunks to threads. The performance of the affinity scheduler is comparable to the \texttt{guided} scheduler until twelve or more threads are used where the performance begins to diverge. The test with 24 threads shows that performance is comparable to the serial execution when too many threads are used. The decrease in performance will be due to the use of the \texttt{critical} section used when assigning a new chunk to a thread, as described in \S\ref{sect:sharing_work}. When more threads are being used, there will naturally be more waiting time for each thread to enter the \texttt{critical} section to be assigned a new chunk. This increases the amount of idle time each thread experiences whilst it waits for a new chunk resulting in less efficient use of the hardware and poorer performance gain. A potential way to combat this slow down is discussed briefly in \S\ref{sect:perf_increase}. The imbalanced nature of loop 1 will also cause threads executing the upper end of the iterations to finish their iterations first. Due to this, these threads will finish their local set of iterations quickly and thus threads will rapidly become available to take work from another thread. This can create a long queue for threads when they are trying to take work from other threads iterations. 
				
				However, the scheduler does not always produce consistent result each test run but this appears to be OS and compiler dependent; for example this issue is present on OS X using GCC-7, but isn't present using GCC-5 on Ubuntu Linux. Further investigation showed that one of the issues causing this is the use of the approximation $\pi = 3.142$ when initialising an array of values for loop 1 to process. When using the C math constant \texttt{M\char`_PI}, the output is the same each run. The consistency of the output is also dependent on the number of threads in use. The issue does not occur when one thread is used, however, when four threads are used the output is often correct the first two or three runs, but afterwards becomes inconsistent.
				
				For loop 1, the best scheduler to use is still \texttt{guided} with a chunksize of 4.
				
			\subsubsection{Loop 2} \label{sect:loop2_affinity}
				Fig. \ref{fig:affinity_comparison} shows that there is a performance increase when using the affinity scheduler for all the threads configurations tested. However, the performance of the affinity scheduler falls short when compared to the \texttt{dynamic, 8} scheduler (the best scheduler found for loop 2 with six threads) when less than tweleve threads are used. The performance gap is due to the overhead of the affinity scheduler, for example when threads are queuing to enter to \texttt{critical} section to recieve a new chunk of work. However, when twelve or more less are used, the performance gain of the affinity scheduler is greater than that of the \texttt{dynamic} scheduler. This performance gain can most likely be attributed to the data being present in more processor caches resulting in threads being able to access the data without having to queue to access it.
				
				For loop 2, the best scheduler depends on the number of threads being used. If twelve or more threads are in used, then the affinity scheduler should be used. However, if less than twelve threads used then it is better to use the \texttt{dynamic} scheduler with a chunksize of 8.
				
			\subsubsection{Improvements} \label{sect:perf_increase}
				To potentially improve the performance of the scheduler, the use of OMP locks should be used instead of \texttt{critical} sections, as mentioned at the end of \S\ref{sect:loop1_affinity}. By using locks, the elements of the \texttt{thread\char`_lowers} and \texttt{thread\char`_uppers} arrays can be protected from being accessed and updated by multiple threads at once. This is advantageous as it would allow threads to update multiple values of the boundary arrays at once, as long as threads are not trying to update the same element at once. This will reduce the waiting time for a thread to be assigned a new chunk of work and thus improve the efficiency of the scheduler. 
	
	\section{Conclusion}
		It was found that the best schedule to use for an imbalanced loop similar to loop 1 is \texttt{guided} with a chunksize of 4, when using six threads. However, if the number of iterations or execution time is small, such as with loop 1, then enough threads have to be used when running the program otherwise performance is likely to decrease when running in parallel due to scheduling overheads. This is due to ``death by synchronisation'' caused by the threads communicating. For example, parallel computation only begins to benefit runtime for loop 1 when six or more threads are used. The chunksize was found to have little effect on the performance of the \texttt{guided} and \texttt{dynamic} schedulers for loop 1. However, chunksize affected the performance of the \texttt{static} scheduler dramatically and small chunksizes are advised to be used if the \texttt{static} scheduler is used for imbalanced loops.
		
		For loop 2 it was found that the best schedule to use depended on the number of threads available. When more than six threads are used, the affinity scheduler was found to have the greatest performance increase. However when using six threads or less, it was found that the \texttt{dynamic} scheduler with a chunksize of 8 is best. The \texttt{guided} schedule was unaffected by changing the minimum chunksize, however it was found that the \texttt{static} and \texttt{dynamic} schedulers provide the greatest performance gain when smaller chunksizes were used. But, it must be noted that chunksizes larger and smaller than a chunksize of 4 cause poorer performance for the \texttt{static} schedule. 
		
		The affinity scheduler developed is shown to have limited applications of when it is the best scheduler to use for the most performance gain. In loop 1, the scheduler performed poorly compared to the \texttt{guided} scheduler due to the overhead of synchronising threads. For loop 2, the affinity scheduler again performs poorer than the OMP \texttt{dynamic} scheduler when less than twelve threads are used. However, when twelve or more threads are used the affinity scheduler performs significantly better due to better data affinity for the threads. Performance of the scheduler could be improved by using OMP locks instead of a \texttt{critical} section.
		
	\newpage
	\section{Appendix} \label{sect:appendix}
	
		\begin{figure}[!h]
			\centering
			\includegraphics[scale=0.8]{affinity_flow.pdf}
			\caption{A flow chart for basic operation of the function \texttt{affinity\char`_loop}.}
			\label{fig:affinity_loop_flow}
		\end{figure}	
		
		\newpage
		\begin{figure}[]
			\centering
			\includegraphics[scale=0.9]{share_iterations.pdf}
			\caption{A flow chart for basic operation of the function \texttt{share\char`_iterations}.}
			\label{fig:ashare_iterations_flow}
		\end{figure}	
		
\end{document}